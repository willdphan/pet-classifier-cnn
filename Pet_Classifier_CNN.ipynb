{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willdphan/pet-classifier-cnn/blob/main/Pet_Classifier_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pet Classifier CNN\n",
        "\n",
        "\n",
        "This code implements a deep learning model based on the `ResNet-50` architecture for image classification. The model is trained and tested on a dataset containing images of cats and dogs. The training process involves iterating over mini-batches of images, calculating losses, and updating the model parameters using the `Adam optimizer`. After training, the model is evaluated on a separate test dataset to measure its accuracy.\n",
        "\n",
        "During the testing phase, the code loads the test dataset and iterates over the images. Each image is passed through the trained model, which predicts whether the image contains a cat or a dog. The predicted class (cat or dog) is then printed for each image.\n",
        "\n",
        "> ## Steps to CNN\n",
        "\n",
        ">>[Import Libraries](#scrollTo=ZKJON9aGFwmG)\n",
        "\n",
        ">>[Set Device and Retrieve Data](#scrollTo=iOdOihFWFsf_)\n",
        "\n",
        ">>[Split Training Dataset](#scrollTo=Dhc5MmpQHE_7)\n",
        "\n",
        ">>[Preprocess with ImageLoader Function](#scrollTo=iPJdkKQocVwk)\n",
        "\n",
        ">>[Transform Datasets and Load into Preprocessor](#scrollTo=KXPGe3-Fke97)\n",
        "\n",
        ">>[Load Preprocessed Datasets into DataLoader for Batch Training](#scrollTo=coFdrtxtlYKP)\n",
        "\n",
        ">>[Load Pre-Trained Model](#scrollTo=j1heu6iUm1rk)\n",
        "\n",
        ">>[Load Loss Function and Optimizer](#scrollTo=ZiuaaZMrIO5Y)\n",
        "\n",
        ">>[Train Function](#scrollTo=3s8QcW8fsTao)\n",
        "\n",
        ">>[Test Function](#scrollTo=QMQDEmdRtYzG)\n",
        "\n",
        ">>[Save checkpoint](#scrollTo=_HMqTmiIJ4o7)\n",
        "\n",
        ">>[Preprocess Untouched Testing Data and Get Results](#scrollTo=rExehwpHKPnQ)\n",
        "\n",
        ">>[Create Function to Try with Random Images](#scrollTo=birF5CVvKt1L)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "rZwEy77HY8mN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKJON9aGFwmG"
      },
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z5bbDAjaFacE"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
        "import torch.nn.functional as F # All functions that don't have any parameters\n",
        "from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOdOihFWFsf_"
      },
      "source": [
        "### Set Device and Retrieve Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xzx0RcjkFnpv"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GzyrEKHhVvNH"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/archive.zip'\n",
        "extract_path = '/content/sample_data'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Training Dataset\n",
        "\n",
        "Notice how we are splitting the training dataset and not the testing dataset. The untouched testing dataset will be used to compare to training predictions."
      ],
      "metadata": {
        "id": "Dhc5MmpQHE_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zvAXw7cqYuwi"
      },
      "outputs": [],
      "source": [
        "dataset = ImageFolder(\"/content/sample_data/training_set/training_set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LN3K9DzLIkbd"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import RandomState\n",
        "train_data, test_data, train_label, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPJdkKQocVwk"
      },
      "source": [
        "### Preprocess with ImageLoader Function\n",
        "\n",
        "`init` function filters the original dataset and remove any images that are not in the RGB color channel format using `checkChannel` within the `ImageLoader` class.\n",
        "`ImageLoader` is used to load or preprocess the images for testing/training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "feqLGHAmKxXR"
      },
      "outputs": [],
      "source": [
        "class ImageLoader(Dataset):\n",
        "    # note that the transform method is optional\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = self.checkChannel(dataset) # some images are CMYK, Grayscale, check only RGB\n",
        "        self.transform = transform\n",
        "\n",
        "    # get total number of images in dataset\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    # allows indexing and retrieving items from the dataset\n",
        "    def __getitem__(self, item):\n",
        "        # retrieves the item (contains image path and class category) at the given index from the dataset\n",
        "        # the 0 retrieves the image path (file name)\n",
        "        image = Image.open(self.dataset[item][0])\n",
        "        # retrieves the class category as second item\n",
        "        classCategory = self.dataset[item][1]\n",
        "        # if transform is not 'none' it applies transformation and\n",
        "        # returns transformed image and class category as tuple\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, classCategory\n",
        "\n",
        "    # filters out images in the dataset that are not in RGB format\n",
        "    # returns a new dataset that only contains RGB images\n",
        "    def checkChannel(self, dataset):\n",
        "        # initialize array of RBD formatted images\n",
        "        datasetRGB = []\n",
        "        # loops over images in dataset\n",
        "        for index in range(len(dataset)):\n",
        "            # checks if bands contain RGB, if so then add to datasetRGB[]\n",
        "            if (Image.open(dataset[index][0]).getbands() == (\"R\", \"G\", \"B\")): # Check Channels\n",
        "                datasetRGB.append(dataset[index])\n",
        "        return datasetRGB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXPGe3-Fke97"
      },
      "source": [
        "### Transform Datasets and Load into Preprocessor\n",
        "\n",
        "The values for Normalize are `[0.5]*3` because it's used to create a list of three elements that represent 3 color channels (R, G, B). For each color channel (R, G, B) in the image, the mean and standard deviation values of `0.5` are used. This means that the pixel values of each channel will be subtracted by `0.5` and then divided by `0.5` during the normalization process. This centers the pixel values around zero and scales them to a range of approximately -1 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0bhqBDSdhtPx"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([                                         # mean   # std dev\n",
        "    transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "]) # train transform\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "]) # test transform\n",
        "\n",
        "train_dataset = ImageLoader(train_data, train_transform)\n",
        "test_dataset = ImageLoader(test_data, test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFdrtxtlYKP"
      },
      "source": [
        "### Load Preprocessed Datasets into DataLoader for Batch Training\n",
        "\n",
        "`DataLoader` enables batches of data and whether data should be shuffled during the training or testing process. Used with transformed and preprocessed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pTFnD6balP_3"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1heu6iUm1rk"
      },
      "source": [
        "### Load Pre-Trained Model\n",
        "\n",
        "Load the pre-trained ResNet model. The loop iterates over all parameters in the model and sets their `requires_grad` attribute to False.\n",
        "\n",
        "By doing so, it freezes the parameters of the pre-trained layers, meaning their weights will not be updated during the training process - This is a common practice in transfer learning to prevent the gradients from propagating through these layers.\n",
        "\n",
        "`model.fc = nn.Linear(num_ftrs, 2)` replaces the last fully connected layer (`model.fc`) with a new `nn.Linear` layer. The new layer has `num_ftrs` input features (matching the previous layer's output features) and 2 output features, indicating a classification task with 2 classes (dog and cat). By replacing the classifier layer, we can adapt the model to the our classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDYXzMenmNYt",
        "outputId": "e9997988-d916-4ec3-8be3-9cb5382f4268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 239MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# display progress bars\n",
        "from tqdm import tqdm\n",
        "# import torch models\n",
        "from torchvision import models\n",
        "# load pretrained resnet model and modify\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# freeze params of pre-trained layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace last fc layer of pre-trained model\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "# model to device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "ZiuaaZMrIO5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aa5wfn_wpR45"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s8QcW8fsTao"
      },
      "source": [
        "### Train Function\n",
        "\n",
        "The `train` function is responsible for training a deep learning model.\n",
        "\n",
        "1. Initializes a list to track the losses during training.\n",
        "2. Sets the model in training mode.\n",
        "3. Creates a progress bar to visualize the training progress.\n",
        "4. Iterates over the data batches.\n",
        "  *   Moves data and targets to the device for computation.\n",
        "  *   Computes predicted scores using the model.\n",
        "  *   Calculates the loss between the scores and targets.\n",
        "  *   Resets gradients to zero.\n",
        "  *   Performs backpropagation and updates the model's parameters.\n",
        "  *   Obtains predicted class labels.\n",
        "  *   Updates the progress bar and displays the current loss.\n",
        "5. Saves the model's state and optimizer's state at the end of each epoch.\n",
        "\n",
        "TLDR; The `train` function trains the model by iteratively updating its parameters based on computed gradients, tracking the losses, and saving the model's progress for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AIQW3JKbnupc"
      },
      "outputs": [],
      "source": [
        "# Train Function\n",
        "# use train function over number of epoch with model\n",
        "def train(num_epoch, model):\n",
        "    for epoch in range(0, num_epoch):\n",
        "#         current_loss = 0.0\n",
        "#         current_corrects = 0\n",
        "\n",
        "        losses = [] # keep track of total losses\n",
        "        model.train() # set model in training mode\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n",
        "\n",
        "        for batch_idx, (data, targets) in loop:\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "            scores = model(data)\n",
        "\n",
        "            loss = criterion(scores, targets) # calculate loss with CE\n",
        "            optimizer.zero_grad() # reset gradient params to 0\n",
        "            losses.append(loss) # add the loss to the total loss\n",
        "            loss.backward() # computes the gradients of the loss with respect to the model's params with backpropagation\n",
        "            optimizer.step() # updates the model's params with an optimization step based on the computed gradients\n",
        "            _, preds = torch.max(scores, 1) # returns max value and index associated with value\n",
        "\n",
        "            # updates progress bar description with current epoch and batch progress\n",
        "            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n",
        "            # updates progress bar with current loss value for batch\n",
        "            loop.set_postfix(loss=loss.data.item())\n",
        "\n",
        "        # saves the model's state, model's parameters and optimizer's state\n",
        "        # saves the model's progress at the end of each epoch.\n",
        "        torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    }, 'checpoint_epoch_'+str(epoch)+'.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQDEmdRtYzG"
      },
      "source": [
        "### Test Function\n",
        "\n",
        "`model.eval()` is a kind of switch for some specific layers/parts of the model that behave differently. During training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. `.eval()` turns them off them during model evaluation\n",
        "\n",
        "In addition, the common practice for evaluating/validation is using `torch.no_grad()` in pair with `model.eval()` to turn off gradients computation.\n",
        "\n",
        "Format of `test()` is similar to `train()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z6FFfNyTn8Q7"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    # set to evaluation mode and initialize total test loss and correct\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            # output of model\n",
        "            output = model(x)\n",
        "            # Get Prediction number and Class (Dog/Cat)\n",
        "            _, predictions = torch.max(output, 1)\n",
        "            # update total correct\n",
        "            correct += (predictions == y).sum().item()\n",
        "            # set the test loss\n",
        "            test_loss = criterion(output, y)\n",
        "\n",
        "    # calc avg loss per sample in test dataset\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    # average loss and accuracy of the model on the test dataset\n",
        "    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n",
        "    len(test_loader.dataset), \"  \", int(correct / len(test_loader.dataset) * 100), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7myJKrfHuKsc"
      },
      "source": [
        "Call the train and test function. By using the if `__name__ == \"__main__\"` condition, you ensure that these functions are only executed when the script is run directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSYmuPRZtziM",
        "outputId": "187a8ea2-ee3d-487e-cc0b-39c289113abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 process: 99: 100%|██████████| 101/101 [00:49<00:00,  2.04it/s, loss=3.96]\n",
            "Epoch 2/5 process: 99: 100%|██████████| 101/101 [00:42<00:00,  2.36it/s, loss=1.49e-7]\n",
            "Epoch 3/5 process: 99: 100%|██████████| 101/101 [00:40<00:00,  2.48it/s, loss=0.00606]\n",
            "Epoch 4/5 process: 99: 100%|██████████| 101/101 [00:40<00:00,  2.47it/s, loss=3.55e-6]\n",
            "Epoch 5/5 process: 99: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=0.00484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss:  tensor(0.0004, device='cuda:0')   Accuracy:  1523  /  1601    95 %\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train(5, model) # train\n",
        "    test() # test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save checkpoint\n",
        "\n",
        "The provided code snippet loads a checkpoint file named `\"checpoint_epoch_4.pt\"`. It attempts to load the saved state of a model and its corresponding optimizer from the checkpoint. The model's state is loaded using model.`load_state_dict(checkpoint[\"model_state_dict\"])`, and the optimizer's state is loaded using `optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])`. After loading the checkpoint, the message \"Loading checkpoint\" is printed to indicate that the process has been completed."
      ],
      "metadata": {
        "id": "_HMqTmiIJ4o7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xa6_2Av2wjCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e2d0a7-fc6e-47d9-f1a0-0ee4400f09d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Loading checkpoint\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(\"./checpoint_epoch_4.pt\") # Try to load last checkpoint\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "print(\"...Loading checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Untouched Testing Data and Get Results\n",
        "\n",
        "Preprocess the testing dataset and load it into the DataLoader. Set the model to evaluation mode to avoid modifying weights and get results for untouched testing dataset.\n",
        "\n",
        "TLDR; test the model with the testing data. Track total Dogs and Cats."
      ],
      "metadata": {
        "id": "rExehwpHKPnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(\"/content/sample_data/test_set\",\n",
        "                     transform=transforms.Compose([\n",
        "                         transforms.Resize((224, 224)),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "                     ]))\n",
        "print(dataset)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle = False)"
      ],
      "metadata": {
        "id": "xk6LEoVDIry-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53e59ac-b0a7-4f91-98cb-18bd2a2ea828"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 2023\n",
            "    Root location: /content/sample_data/test_set\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for j, (data, labels) in enumerate(dataloader):\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    Dogs = 0\n",
        "    Cats = 0\n",
        "    for data, target in dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        if predicted[0] == 1:\n",
        "          Dogs += 1\n",
        "        else:\n",
        "          Cats +=1\n",
        "    print(f\"Total Dogs: {Dogs}\")\n",
        "    print(f\"Total Cats: {Cats}\")"
      ],
      "metadata": {
        "id": "j0RObZtXIu31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca80f72-9711-4f07-dc25-d3ccf7928173"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Dogs: 1092\n",
            "Total Cats: 931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Function to Try with Random Images\n",
        "\n",
        "Takes a filepath/file name as input. The function opens an image file and converts it to RGB format.\n",
        "\n",
        "It then applies a series of transformations, including resizing the image to 224x224 pixels, converting it to a tensor, and normalizing the pixel values. The transformed image is stored in a tensor and wrapped with a dimension of size one.\n",
        "\n",
        "The function uses a `DataLoader` to load the transformed image tensor. It then iterates over the `DataLoader`, passing each batch of data to the model for prediction. The predicted class labels are obtained by finding the maximum value along the predicted scores axis. The predicted class label is printed as either \"Dog\" or \"Cat\" based on the value of `preds[0]`.\n",
        "\n",
        "Overall, the function performs image preprocessing, passes the preprocessed image through a model for prediction, and prints the predicted class label."
      ],
      "metadata": {
        "id": "birF5CVvKt1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomImagePrediction(filepath):\n",
        "    # opens file and converts to RGB\n",
        "    img_array = Image.open(filepath).convert(\"RGB\")\n",
        "    # set transformation var\n",
        "    data_transforms=transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ])\n",
        "\n",
        "    # set new transformed image with above tranformation var\n",
        "    img = data_transforms(img_array).unsqueeze(dim=0) # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    # use DataLoader of img to help batch for training\n",
        "    load = DataLoader(img)\n",
        "\n",
        "    # loops over img DataLoader\n",
        "    for x in load:\n",
        "        x= x.to(device)\n",
        "        # get result of model\n",
        "        pred = model(x)\n",
        "        # get result and class\n",
        "        _, preds = torch.max(pred, 1)\n",
        "\n",
        "        # print the class - Dog or Cat\n",
        "        print(f\"Class (Dog/Cat): {preds}\")\n",
        "\n",
        "        # if 1, then it's Dog, if 0, then it's Cat\n",
        "        if preds[0] == 1:\n",
        "          print(f\"Prediction: Dog\")\n",
        "        else:\n",
        "          print(f\"Prediction: Cat\")"
      ],
      "metadata": {
        "id": "vE5G9MEkIznG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    RandomImagePrediction(\"/content/sample_data/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\") # dog image"
      ],
      "metadata": {
        "id": "k7JXEd8TI1uE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc60bb47-9828-4199-e2b5-90447461c7bb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class (Dog/Cat): tensor([1], device='cuda:0')\n",
            "Prediction: Dog\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOXxP6+J4wL8wZEERSdAzcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}